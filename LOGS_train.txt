# ==================== 参数设置区域 ====================
# 随机种子
SEED <- 123
# 学习曲线专用种子
LEARNING_CURVE_SEED <- 234

# 交叉验证折数
CV_FOLDS <- 5

# 工作路径
WORK_DIR <- "D:/2/"
OUTPUT_DIR <- "Logistic_LASSO"

# 创建输出目录（使用递归创建，确保路径存在）
if (!dir.exists(OUTPUT_DIR)) {
  dir.create(OUTPUT_DIR, recursive = TRUE, showWarnings = FALSE)
}

# 模型选择的评价指标 (可选: "Accuracy", "AUC", "F1", "Precision", "Recall")
SELECTION_METRIC <- "AUC"

# ==================== 新增：早停参数 ====================
# 是否启用早停
ENABLE_EARLY_STOPPING <- TRUE

# 早停监控指标 ("AUC", "Accuracy", "F1" 等)
EARLY_STOP_METRIC <- "AUC"

# 训练集和验证集性能差距阈值（当差距超过此值时触发早停）
OVERFITTING_THRESHOLD <- 0.05

# 早停耐心值（连续多少次参数组合出现过拟合才真正停止）
EARLY_STOP_PATIENCE <- 3

# 最小训练轮数（至少尝试多少个参数组合）
MIN_ITERATIONS <- 5

# 是否在早停时保存诊断信息
SAVE_EARLY_STOP_DIAGNOSTICS <- TRUE

# ==================== 原有参数 ====================
# 逻辑回归参数网格（使用glmnet进行正则化）
LR_GRID <- expand.grid(
  alpha = c(0, 0.05, 0.1, 0.5, 1),              # 0=Ridge, 0.5=Elastic Net, 1=Lasso
  lambda = 10^seq(-1, 3, length = 25)    # 正则化强度
)

# 学习曲线采样比例（用于过拟合诊断）
LEARNING_CURVE_SAMPLES <- c(0.2, 0.4, 0.6, 0.8, 1.0)

# ==================== 新增：特征选择可视化参数 ====================
# 特征选择阈值（特征在多少比例的折中被选中才算"选中"）
FEATURE_SELECTION_THRESHOLD <- 0.8  # 80%，5折中至少4折

# 系数阈值（绝对值小于此值视为未选中，即系数为0）
COEFFICIENT_THRESHOLD <- 1e-6  # 接近0的阈值

# ==================== 加载库 ====================
library(readxl)
library(glmnet)
library(writexl)
library(caret)
library(pROC)
library(ggplot2)
library(reshape2)
library(nnet)  # 用于多分类逻辑回归

# ==================== 设置工作环境 ====================
setwd(WORK_DIR)
set.seed(SEED)

# 验证评价指标设置
valid_metrics <- c("Accuracy", "AUC", "F1", "Precision", "Recall")
if (!(SELECTION_METRIC %in% valid_metrics)) {
  stop(paste0("错误: SELECTION_METRIC 必须是以下之一: ", 
              paste(valid_metrics, collapse = ", ")))
}

if (!(EARLY_STOP_METRIC %in% valid_metrics)) {
  stop(paste0("错误: EARLY_STOP_METRIC 必须是以下之一: ", 
              paste(valid_metrics, collapse = ", ")))
}

cat("==================== 模型配置 ====================\n")
cat("选择指标:", SELECTION_METRIC, "\n")
cat("交叉验证折数:", CV_FOLDS, "\n")
cat("随机种子:", SEED, "\n")
cat("学习曲线种子:", LEARNING_CURVE_SEED, "\n")
if (ENABLE_EARLY_STOPPING) {
  cat("早停功能: 启用\n")
  cat("早停监控指标:", EARLY_STOP_METRIC, "\n")
  cat("过拟合阈值:", OVERFITTING_THRESHOLD, "\n")
  cat("早停耐心值:", EARLY_STOP_PATIENCE, "\n")
} else {
  cat("早停功能: 禁用\n")
}
cat("==============================================\n\n")


# ==================== 读取数据 ====================
data <- read_excel("1.xlsx")
feature_names <- names(data)

# 提取特征和目标变量
y <- data[[1]]
X <- as.data.frame(data[, -1])

# 转换目标变量
y_factor <- as.factor(y)
y_numeric <- as.integer(y_factor) - 1
num_class <- length(levels(y_factor))

cat("数据加载完成\n")
cat("样本数:", nrow(X), "\n")
cat("特征数:", ncol(X), "\n")
cat("类别数:", num_class, "\n\n")

# ==================== 计算ROC-AUC的函数 ====================
calculate_auc <- function(actual, prob_matrix) {
  if (ncol(prob_matrix) == 2) {
    # 二分类
    roc_obj <- roc(actual, prob_matrix[, 2], quiet = TRUE)
    return(as.numeric(auc(roc_obj)))
  } else {
    # 多分类：使用macro-average AUC
    auc_values <- numeric(ncol(prob_matrix))
    for (i in 1:ncol(prob_matrix)) {
      binary_actual <- ifelse(actual == (i-1), 1, 0)
      roc_obj <- roc(binary_actual, prob_matrix[, i], quiet = TRUE)
      auc_values[i] <- as.numeric(auc(roc_obj))
    }
    return(mean(auc_values))
  }
}

# ==================== 计算综合评价指标 ====================
calculate_metrics <- function(actual, predicted, prob_matrix) {
  actual <- factor(actual, levels = levels(predicted))
  cm <- confusionMatrix(predicted, actual)
  
  # 计算AUC
  auc_score <- calculate_auc(as.numeric(actual) - 1, prob_matrix)
  
  if (nlevels(actual) > 2) {
    metrics <- list(
      Accuracy = cm$overall["Accuracy"],
      Precision = mean(cm$byClass[,"Pos Pred Value"], na.rm = TRUE),
      Recall = mean(cm$byClass[,"Sensitivity"], na.rm = TRUE),
      F1 = mean(cm$byClass[,"F1"], na.rm = TRUE),
      Kappa = cm$overall["Kappa"],
      Mean_Sensitivity = mean(cm$byClass[,"Sensitivity"], na.rm = TRUE),
      Mean_Specificity = mean(cm$byClass[,"Specificity"], na.rm = TRUE),
      Mean_Pos_Pred_Value = mean(cm$byClass[,"Pos Pred Value"], na.rm = TRUE),
      Mean_Neg_Pred_Value = mean(cm$byClass[,"Neg Pred Value"], na.rm = TRUE),
      Mean_Detection_Rate = mean(cm$byClass[,"Detection Rate"], na.rm = TRUE),
      Mean_Balanced_Accuracy = mean(cm$byClass[,"Balanced Accuracy"], na.rm = TRUE),
      AUC = auc_score
    )
  } else {
    metrics <- list(
      Accuracy = cm$overall["Accuracy"],
      Precision = cm$byClass["Pos Pred Value"],
      Recall = cm$byClass["Sensitivity"],
      F1 = cm$byClass["F1"],
      Kappa = cm$overall["Kappa"],
      Mean_Sensitivity = cm$byClass["Sensitivity"],
      Mean_Specificity = cm$byClass["Specificity"],
      Mean_Pos_Pred_Value = cm$byClass["Pos Pred Value"],
      Mean_Neg_Pred_Value = cm$byClass["Neg Pred Value"],
      Mean_Detection_Rate = cm$byClass["Detection Rate"],
      Mean_Balanced_Accuracy = cm$byClass["Balanced Accuracy"],
      AUC = auc_score
    )
  }
  
  return(metrics)
}

# ==================== 新增：早停检测函数 ====================
check_early_stopping <- function(train_metric, val_metric, threshold) {
  gap <- train_metric - val_metric
  is_overfitting <- gap > threshold
  return(list(
    is_overfitting = is_overfitting,
    gap = gap,
    train_metric = train_metric,
    val_metric = val_metric
  ))
}

# ==================== 学习曲线分析（过拟合诊断）====================
analyze_learning_curve <- function(X, y, best_params) {
  cat("\n开始学习曲线分析...\n")
  
  # 设置学习曲线专用种子
  set.seed(LEARNING_CURVE_SEED)
  
  learning_results <- list()
  
  for (sample_ratio in LEARNING_CURVE_SAMPLES) {
    cat(sprintf("  样本比例: %.1f%%\n", sample_ratio * 100))
    
    # 随机采样
    n_samples <- floor(nrow(X) * sample_ratio)
    sample_idx <- sample(1:nrow(X), n_samples)
    
    X_sample <- X[sample_idx, ]
    y_sample <- y_factor[sample_idx]
    
    # 划分训练集和验证集
    train_idx <- sample(1:n_samples, floor(n_samples * 0.8))
    val_idx <- setdiff(1:n_samples, train_idx)
    
    # 训练模型
    if (num_class > 2) {
      # 多分类逻辑回归
      model <- multinom(y_sample[train_idx] ~ ., 
                       data = X_sample[train_idx, ],
                       trace = FALSE,
                       maxit = 500)
    } else {
      # 二分类逻辑回归（使用glmnet）
      model <- glmnet(
        x = as.matrix(X_sample[train_idx, ]),
        y = y_sample[train_idx],
        family = "binomial",
        alpha = best_params$alpha,
        lambda = best_params$lambda
      )
    }
    
    # 预测
    if (num_class > 2) {
      train_prob <- predict(model, newdata = X_sample[train_idx, ], type = "probs")
      val_prob <- predict(model, newdata = X_sample[val_idx, ], type = "probs")
      if (is.vector(train_prob)) {
        train_prob <- matrix(train_prob, ncol = num_class, byrow = TRUE)
        val_prob <- matrix(val_prob, ncol = num_class, byrow = TRUE)
      }
    } else {
      train_pred <- predict(model, newx = as.matrix(X_sample[train_idx, ]), 
                           type = "response", s = best_params$lambda)
      val_pred <- predict(model, newx = as.matrix(X_sample[val_idx, ]), 
                         type = "response", s = best_params$lambda)
      train_prob <- cbind(1 - train_pred, train_pred)
      val_prob <- cbind(1 - val_pred, val_pred)
    }
    
    # 计算AUC
    train_auc <- calculate_auc(as.numeric(y_sample[train_idx]) - 1, train_prob)
    val_auc <- calculate_auc(as.numeric(y_sample[val_idx]) - 1, val_prob)
    
    learning_results[[as.character(sample_ratio)]] <- list(
      sample_size = n_samples,
      train_auc = train_auc,
      val_auc = val_auc,
      gap = train_auc - val_auc
    )
  }
  
  # 恢复原始种子
  set.seed(SEED)
  
  return(learning_results)
}

# ==================== 绘制学习曲线（灰度）====================
plot_learning_curve <- function(learning_results, output_dir) {
  df <- do.call(rbind, lapply(names(learning_results), function(ratio) {
    data.frame(
      sample_size = learning_results[[ratio]]$sample_size,
      train_auc = learning_results[[ratio]]$train_auc,
      val_auc = learning_results[[ratio]]$val_auc,
      gap = learning_results[[ratio]]$gap
    )
  }))
  
  # 保存数据到Excel
  write_xlsx(list(LearningCurve = df), 
            path = file.path(output_dir, "learning_curve.xlsx"))
  
  # 从输出目录名称提取标题
  plot_title <- gsub("_", " ", basename(output_dir))
  
  p <- ggplot(df, aes(x = sample_size)) +
    geom_line(aes(y = train_auc, linetype = "Training"), size = 1.2, color = "black") +
    geom_point(aes(y = train_auc, shape = "Training"), size = 3, color = "black") +
    geom_line(aes(y = val_auc, linetype = "Validation"), size = 1.2, color = "black") +
    geom_point(aes(y = val_auc, shape = "Validation"), size = 3, color = "black") +
    scale_linetype_manual(values = c("Training" = "solid", "Validation" = "dashed"),
                         name = "Dataset") +
    scale_shape_manual(values = c("Training" = 16, "Validation" = 17),
                      name = "Dataset") +
    labs(
      title = plot_title,
      x = "Training Set Size",
      y = "AUC Score"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      legend.position = "bottom",
      legend.title = element_text(size = 10, face = "bold"),
      legend.text = element_text(size = 9),
      panel.grid.major = element_line(color = "grey90"),
      panel.grid.minor = element_blank(),
      axis.text = element_text(color = "black", size = 10),
      axis.title = element_text(color = "black", face = "bold", size = 11)
    )
  
  # 保存为TIFF格式，600 DPI
  ggsave(file.path(output_dir, "learning_curve.tiff"), p, 
         width = 8, height = 6, dpi = 600, compression = "lzw")
  cat("学习曲线已保存（TIFF格式，600 DPI，灰度）\n")
}

# ==================== 绘制交叉验证性能箱线图（灰度）====================
plot_cv_performance <- function(all_metrics, output_dir) {
  metrics_df <- do.call(rbind, lapply(seq_along(all_metrics), function(i) {
    data.frame(
      Fold = paste0("Fold", i),
      AUC = all_metrics[[i]]$AUC,
      Accuracy = all_metrics[[i]]$Accuracy,
      Precision = all_metrics[[i]]$Precision,
      Recall = all_metrics[[i]]$Recall,
      F1 = all_metrics[[i]]$F1
    )
  }))
  
  # 保存数据到Excel
  write_xlsx(list(CV_Performance = metrics_df), 
            path = file.path(output_dir, "cv_performance.xlsx"))
  
  # 按指定顺序设置因子水平
  metrics_long <- melt(metrics_df, id.vars = "Fold", 
                      variable.name = "Metric", value.name = "Score")
  metrics_long$Metric <- factor(metrics_long$Metric, 
                                levels = c("AUC", "Accuracy", "Precision", "Recall", "F1"))
  
  # 从输出目录名称提取标题
  plot_title <- gsub("_", " ", basename(output_dir))
  
  p <- ggplot(metrics_long, aes(x = Metric, y = Score)) +
    geom_boxplot(alpha = 0.7, fill = "grey70", color = "black") +
    geom_jitter(width = 0.2, alpha = 0.5, size = 2, color = "black") +
    stat_summary(fun = mean, geom = "point", shape = 23, size = 3, 
                 fill = "black", color = "black") +
    labs(
      title = plot_title,
      subtitle = "Black diamonds indicate mean values",
      x = "Evaluation Metric",
      y = "Score"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 10),
      panel.grid.major = element_line(color = "grey80"),
      panel.grid.minor = element_blank(),
      axis.text = element_text(color = "black", size = 10),
      axis.title = element_text(color = "black", face = "bold", size = 11)
    ) +
    ylim(0, 1)
  
  # 保存为TIFF格式，600 DPI
  ggsave(file.path(output_dir, "cv_performance.tiff"), p, 
         width = 8, height = 6, dpi = 600, compression = "lzw")
  cat("交叉验证性能图已保存（TIFF格式，600 DPI，灰度）\n")
}

# ==================== 绘制DCA曲线（灰度）====================
plot_dca <- function(all_predictions, y_factor, output_dir) {
  # 合并所有预测
  all_preds <- do.call(rbind, all_predictions)
  
  # 提取概率（假设是二分类，取第二列）
  if (num_class == 2) {
    prob_col <- paste0("Prob_", levels(y_factor)[2])
    predicted_prob <- all_preds[[prob_col]]
    actual <- ifelse(all_preds$Actual == levels(y_factor)[2], 1, 0)
    
    # 计算不同阈值下的净收益
    thresholds <- seq(0, 1, by = 0.01)
    net_benefit_model <- numeric(length(thresholds))
    net_benefit_all <- numeric(length(thresholds))
    
    for (i in seq_along(thresholds)) {
      threshold <- thresholds[i]
      
      # 模型预测
      predicted <- ifelse(predicted_prob >= threshold, 1, 0)
      tp <- sum(predicted == 1 & actual == 1)
      fp <- sum(predicted == 1 & actual == 0)
      
      # 净收益 = (TP/n) - (FP/n) * (threshold/(1-threshold))
      n <- length(actual)
      if (threshold < 1) {
        net_benefit_model[i] <- (tp/n) - (fp/n) * (threshold/(1-threshold))
        net_benefit_all[i] <- (sum(actual)/n) - ((n-sum(actual))/n) * (threshold/(1-threshold))
      } else {
        net_benefit_model[i] <- 0
        net_benefit_all[i] <- 0
      }
    }
    
    # 创建数据框用于保存
    dca_data <- data.frame(
      Threshold = thresholds,
      Model_Net_Benefit = net_benefit_model,
      Treat_All_Net_Benefit = net_benefit_all,
      Treat_None_Net_Benefit = 0
    )
    
    # 保存数据到Excel
    write_xlsx(list(DCA_Data = dca_data), 
              path = file.path(output_dir, "dca_curve.xlsx"))
    
    # 绘图用的数据框
    dca_df <- data.frame(
      Threshold = rep(thresholds, 3),
      NetBenefit = c(net_benefit_model, net_benefit_all, rep(0, length(thresholds))),
      Strategy = rep(c("Model", "Treat All", "Treat None"), each = length(thresholds))
    )
    
    p <- ggplot(dca_df, aes(x = Threshold, y = NetBenefit, linetype = Strategy)) +
      geom_line(size = 1.2, color = "black") +
      scale_linetype_manual(values = c(
        "Model" = "solid", 
        "Treat All" = "dashed", 
        "Treat None" = "dotted"
      )) +
      labs(
        title = "Decision Curve Analysis",
        x = "Threshold Probability",
        y = "Net Benefit"
      ) +
      theme_bw() +
      theme(
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        legend.position = "bottom",
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9),
        panel.grid.major = element_line(color = "grey90"),
        panel.grid.minor = element_blank(),
        axis.text = element_text(color = "black", size = 10),
        axis.title = element_text(color = "black", face = "bold", size = 11)
      )
    
    # 保存为TIFF格式，600 DPI
    ggsave(file.path(output_dir, "dca_curve.tiff"), p, 
           width = 8, height = 6, dpi = 600, compression = "lzw")
    cat("DCA曲线已保存（TIFF格式，600 DPI，灰度）\n")
  }
}

# ==================== 新增：绘制早停诊断图（灰度）====================
plot_early_stop_diagnostics <- function(early_stop_history, output_dir) {
  # 检查是否有有效的历史记录
  if (length(early_stop_history) == 0) {
    cat("  未检测到早停历史记录，跳过诊断图绘制\n")
    return()
  }
  
  # 过滤掉NULL值
  early_stop_history <- early_stop_history[!sapply(early_stop_history, is.null)]
  
  if (length(early_stop_history) == 0) {
    cat("  早停历史记录为空，跳过诊断图绘制\n")
    return()
  }
  
  # 提取数据
  iterations <- seq_along(early_stop_history)
  train_metrics <- sapply(early_stop_history, function(x) {
    if (is.null(x) || is.null(x$train_metric)) return(NA)
    return(x$train_metric)
  })
  val_metrics <- sapply(early_stop_history, function(x) {
    if (is.null(x) || is.null(x$val_metric)) return(NA)
    return(x$val_metric)
  })
  gaps <- sapply(early_stop_history, function(x) {
    if (is.null(x) || is.null(x$gap)) return(NA)
    return(x$gap)
  })
  
  # 检查是否有有效数据
  if (all(is.na(train_metrics)) || all(is.na(val_metrics))) {
    cat("  早停数据无效，跳过诊断图绘制\n")
    return()
  }
  
  # 保存数据到Excel
  early_stop_data <- data.frame(
    Iteration = iterations,
    Train_Metric = train_metrics,
    Validation_Metric = val_metrics,
    Gap = gaps,
    Is_Overfitting = gaps > OVERFITTING_THRESHOLD
  )
  
  write_xlsx(list(EarlyStopMetrics = early_stop_data), 
            path = file.path(output_dir, "early_stop_metrics.xlsx"))
  
  # 创建数据框
  df <- data.frame(
    Iteration = rep(iterations, 3),
    Value = c(train_metrics, val_metrics, gaps),
    Type = rep(c("Train", "Validation", "Gap"), each = length(iterations))
  )
  
  # 绘制训练和验证指标（灰度）
  p1 <- ggplot(df[df$Type != "Gap", ], aes(x = Iteration, y = Value, linetype = Type, shape = Type)) +
    geom_line(size = 1.2, color = "black") +
    geom_point(size = 3, color = "black") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "grey50") +
    scale_linetype_manual(values = c("Train" = "solid", "Validation" = "dashed"),
                         name = "Dataset") +
    scale_shape_manual(values = c("Train" = 16, "Validation" = 17),
                      name = "Dataset") +
    labs(
      title = paste("Training Process -", EARLY_STOP_METRIC),
      x = "Parameter Combination Index",
      y = EARLY_STOP_METRIC
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      legend.position = "bottom",
      legend.title = element_text(size = 10, face = "bold"),
      legend.text = element_text(size = 9),
      panel.grid.major = element_line(color = "grey90"),
      panel.grid.minor = element_blank(),
      axis.text = element_text(color = "black", size = 10),
      axis.title = element_text(color = "black", face = "bold", size = 11)
    )
  
  # 绘制差距图（灰度）
  gap_data <- data.frame(
    Iteration = iterations,
    Gap = gaps,
    Threshold = OVERFITTING_THRESHOLD
  )
  
  write_xlsx(list(GapData = gap_data), 
            path = file.path(output_dir, "early_stop_gap.xlsx"))
  
  p2 <- ggplot(df[df$Type == "Gap", ], aes(x = Iteration, y = Value)) +
    geom_line(size = 1.2, color = "black") +
    geom_point(size = 3, color = "black") +
    geom_hline(yintercept = OVERFITTING_THRESHOLD, linetype = "dashed", color = "black", size = 1) +
    annotate("text", x = max(iterations) * 0.5, y = OVERFITTING_THRESHOLD, 
             label = paste("Threshold =", OVERFITTING_THRESHOLD), vjust = -0.5, 
             color = "black", fontface = "bold", size = 4) +
    labs(
      title = "Train-Validation Gap",
      x = "Parameter Combination Index",
      y = paste(EARLY_STOP_METRIC, "Gap (Train - Val)")
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      panel.grid.major = element_line(color = "grey90"),
      panel.grid.minor = element_blank(),
      axis.text = element_text(color = "black", size = 10),
      axis.title = element_text(color = "black", face = "bold", size = 11)
    )
  
  # 保存图表为TIFF格式，600 DPI
  ggsave(file.path(output_dir, "early_stop_metrics.tiff"), p1, 
         width = 8, height = 6, dpi = 600, compression = "lzw")
  ggsave(file.path(output_dir, "early_stop_gap.tiff"), p2, 
         width = 8, height = 6, dpi = 600, compression = "lzw")
  
  cat("早停诊断图已保存（TIFF格式，600 DPI，灰度）\n")
}

# ==================== 主训练循环 ====================
cat("开始", CV_FOLDS, "折交叉验证...\n\n")

# 创建交叉验证索引
folds <- createFolds(y_factor, k = CV_FOLDS, list = TRUE, returnTrain = FALSE)

# 存储结果
all_predictions <- list()
all_models <- list()
all_metrics <- list()
all_best_params <- list()
all_early_stop_info <- list()  # 新增：存储早停信息
all_coefficients <- list()  # 新增：存储每折的系数矩阵

# 交叉验证循环
for (fold in 1:CV_FOLDS) {
  cat(sprintf("==================== Fold %d/%d ====================\n", fold, CV_FOLDS))
  
  # 划分训练集和测试集
  test_idx <- folds[[fold]]
  train_idx <- setdiff(1:nrow(X), test_idx)
  
  X_train <- X[train_idx, ]
  y_train <- y_factor[train_idx]
  X_test <- X[test_idx, ]
  y_test <- y_factor[test_idx]
  
  # 在训练集内部再划分一个验证集用于早停（80%训练，20%验证）
  inner_train_size <- floor(length(train_idx) * 0.8)
  inner_train_idx <- sample(1:length(train_idx), inner_train_size)
  inner_val_idx <- setdiff(1:length(train_idx), inner_train_idx)
  
  X_inner_train <- X_train[inner_train_idx, ]
  y_inner_train <- y_train[inner_train_idx]
  X_inner_val <- X_train[inner_val_idx, ]
  y_inner_val <- y_train[inner_val_idx]
  
  # 初始化早停相关变量
  best_val_metric <- -Inf
  patience_counter <- 0
  best_params_fold <- NULL
  early_stopped <- FALSE
  stop_iteration <- 0
  early_stop_history <- list()  # 记录早停过程
  
  # 参数搜索
  for (i in 1:nrow(LR_GRID)) {
    params <- LR_GRID[i, ]
    
    # 训练模型
    if (num_class > 2) {
      # 多分类逻辑回归
      model <- multinom(y_inner_train ~ ., 
                       data = X_inner_train,
                       trace = FALSE,
                       maxit = 500)
      
      # 预测
      inner_train_prob <- predict(model, newdata = X_inner_train, type = "probs")
      inner_val_prob <- predict(model, newdata = X_inner_val, type = "probs")
      
      if (is.vector(inner_train_prob)) {
        inner_train_prob <- matrix(inner_train_prob, ncol = num_class, byrow = TRUE)
        inner_val_prob <- matrix(inner_val_prob, ncol = num_class, byrow = TRUE)
      }
      
      inner_train_pred <- factor(predict(model, newdata = X_inner_train, type = "class"),
                                 levels = levels(y_inner_train))
      inner_val_pred <- factor(predict(model, newdata = X_inner_val, type = "class"),
                              levels = levels(y_inner_val))
    } else {
      # 二分类逻辑回归（使用glmnet）
      model <- glmnet(
        x = as.matrix(X_inner_train),
        y = y_inner_train,
        family = "binomial",
        alpha = params$alpha,
        lambda = params$lambda
      )
      
      # 预测
      inner_train_pred_prob <- predict(model, newx = as.matrix(X_inner_train), 
                                      type = "response", s = params$lambda)
      inner_val_pred_prob <- predict(model, newx = as.matrix(X_inner_val), 
                                    type = "response", s = params$lambda)
      
      inner_train_prob <- cbind(1 - inner_train_pred_prob, inner_train_pred_prob)
      inner_val_prob <- cbind(1 - inner_val_pred_prob, inner_val_pred_prob)
      
      inner_train_pred <- factor(ifelse(inner_train_pred_prob > 0.5, 
                                       levels(y_inner_train)[2], 
                                       levels(y_inner_train)[1]),
                                levels = levels(y_inner_train))
      inner_val_pred <- factor(ifelse(inner_val_pred_prob > 0.5, 
                                     levels(y_inner_val)[2], 
                                     levels(y_inner_val)[1]),
                              levels = levels(y_inner_val))
    }
    
    # 计算指标
    inner_train_metrics <- calculate_metrics(y_inner_train, inner_train_pred, inner_train_prob)
    inner_val_metrics <- calculate_metrics(y_inner_val, inner_val_pred, inner_val_prob)
    
    # 早停检测（仅在启用且超过最小迭代次数后）
    if (ENABLE_EARLY_STOPPING && i >= MIN_ITERATIONS) {
      early_stop_check <- check_early_stopping(
        inner_train_metrics[[EARLY_STOP_METRIC]],
        inner_val_metrics[[EARLY_STOP_METRIC]],
        OVERFITTING_THRESHOLD
      )
      
      # 记录历史
      early_stop_history[[i]] <- early_stop_check
      
      if (early_stop_check$is_overfitting) {
        patience_counter <- patience_counter + 1
        if (patience_counter >= EARLY_STOP_PATIENCE) {
          cat(sprintf("  早停触发: 第%d个参数组合 (连续%d次过拟合)\n", 
                     i, EARLY_STOP_PATIENCE))
          early_stopped <- TRUE
          stop_iteration <- i
          break
        }
      } else {
        patience_counter <- 0  # 重置计数器
      }
    }
    
    # 更新最佳参数（基于验证集的选择指标）
    if (inner_val_metrics[[SELECTION_METRIC]] > best_val_metric) {
      best_val_metric <- inner_val_metrics[[SELECTION_METRIC]]
      best_params_fold <- params
    }
  }
  
  # 保存该折的早停信息
  all_early_stop_info[[fold]] <- list(
    stopped_early = early_stopped,
    stop_iteration = stop_iteration,
    total_iterations = if(early_stopped) stop_iteration else nrow(LR_GRID),
    early_stop_history = early_stop_history
  )
  
  # 使用最佳参数在完整训练集上重新训练
  if (num_class > 2) {
    final_model <- multinom(y_train ~ ., 
                           data = X_train,
                           trace = FALSE,
                           maxit = 500)
    
    final_prob_matrix <- predict(final_model, newdata = X_test, type = "probs")
    if (is.vector(final_prob_matrix)) {
      final_prob_matrix <- matrix(final_prob_matrix, ncol = num_class, byrow = TRUE)
    }
    final_pred_factor <- factor(predict(final_model, newdata = X_test, type = "class"),
                                levels = levels(y_test))
    
    # 提取系数（多分类取第一个类别）
    coef_matrix <- summary(final_model)$coefficients
    coefficients <- abs(coef_matrix[-1, 1])  # 去除截距
  } else {
    final_model <- glmnet(
      x = as.matrix(X_train),
      y = y_train,
      family = "binomial",
      alpha = best_params_fold$alpha,
      lambda = best_params_fold$lambda
    )
    
    final_pred_prob <- predict(final_model, newx = as.matrix(X_test), 
                              type = "response", s = best_params_fold$lambda)
    final_prob_matrix <- cbind(1 - final_pred_prob, final_pred_prob)
    final_pred_factor <- factor(ifelse(final_pred_prob > 0.5, 
                                      levels(y_test)[2], 
                                      levels(y_test)[1]),
                                levels = levels(y_test))
    
    # 提取系数
    coef_values <- as.vector(coef(final_model, s = best_params_fold$lambda))
    coefficients <- abs(coef_values[-1])  # 去除截距
  }
  
  # 保存系数
  all_coefficients[[fold]] <- coefficients
  
  # 计算测试集指标
  fold_metrics <- calculate_metrics(y_test, final_pred_factor, final_prob_matrix)
  
  # 保存结果
  all_metrics[[fold]] <- fold_metrics
  all_best_params[[fold]] <- best_params_fold
  all_models[[fold]] <- final_model
  
  # 保存预测结果（格式与原代码一致）
  result_df <- data.frame(
    RowIndex = test_idx,
    Actual = y[test_idx],
    Predicted = as.character(final_pred_factor)
  )
  
  for (class in 1:ncol(final_prob_matrix)) {
    if (num_class > 2) {
      result_df[[paste0("Prob_", levels(y_factor)[class])]] <- final_prob_matrix[, class]
    } else {
      result_df[[paste0("Prob_", levels(y_factor)[class])]] <- final_prob_matrix[, class]
    }
  }
  
  all_predictions[[paste0("Fold", fold)]] <- result_df
  
  cat(sprintf("Fold %d - %s: %.4f\n", fold, SELECTION_METRIC, fold_metrics[[SELECTION_METRIC]]))
}

# ==================== 保存每折预测结果 ====================
write_xlsx(all_predictions, path = file.path(OUTPUT_DIR, "每折预测结果.xlsx"))

# ==================== 选择最佳折叠 ====================
fold_scores <- sapply(all_metrics, function(x) x[[SELECTION_METRIC]])
best_fold_idx <- which.max(fold_scores)

cat(sprintf("\n找到最佳模型：Fold%d (%s: %.4f)\n", 
            best_fold_idx, SELECTION_METRIC, fold_scores[best_fold_idx]))

# 保存最佳模型
best_model_final <- all_models[[best_fold_idx]]
saveRDS(best_model_final, file = file.path(OUTPUT_DIR, "best_model.rds"))

# ==================== 计算平均指标 ====================
avg_metrics <- list()
metric_names <- names(all_metrics[[1]])
for (name in metric_names) {
  values <- sapply(all_metrics, function(x) x[[name]])
  avg_metrics[[name]] <- mean(values)
}

# ==================== 学习曲线分析 ====================
# 使用最佳折叠的参数
best_params_for_lc <- all_best_params[[best_fold_idx]]
learning_curve_results <- analyze_learning_curve(X, y, best_params_for_lc)
plot_learning_curve(learning_curve_results, OUTPUT_DIR)

# ==================== 绘制交叉验证性能箱线图 ====================
plot_cv_performance(all_metrics, OUTPUT_DIR)

# ==================== 绘制DCA曲线 ====================
plot_dca(all_predictions, y_factor, OUTPUT_DIR)

# ==================== 新增：Alpha参数筛选可视化（灰度）====================
cat("\n生成Alpha参数筛选可视化...\n")

# 提取所有折的最佳alpha值
best_alphas <- sapply(all_best_params, function(x) x$alpha)
alpha_counts <- table(best_alphas)

# 创建alpha分布数据
alpha_dist_df <- data.frame(
  Alpha = as.numeric(names(alpha_counts)),
  Count = as.numeric(alpha_counts),
  Percentage = as.numeric(alpha_counts) / CV_FOLDS * 100,
  Alpha_Type = sapply(as.numeric(names(alpha_counts)), function(a) {
    if (a == 0) return("Ridge")
    else if (a == 1) return("Lasso")
    else return("Elastic Net")
  })
)

# 保存alpha分布数据
write_xlsx(list(AlphaDistribution = alpha_dist_df), 
          path = file.path(OUTPUT_DIR, "alpha_selection.xlsx"))

# 绘制alpha选择分布图（条形图，灰度）
tiff(file.path(OUTPUT_DIR, "alpha_selection.tiff"), 
     width = 8, height = 6, units = "in", res = 600, compression = "lzw")
par(mar = c(5, 5, 4, 2))

# 创建灰度填充
bar_colors <- gray.colors(nrow(alpha_dist_df), start = 0.3, end = 0.7)

barplot(alpha_dist_df$Count,
        names.arg = paste0("α=", alpha_dist_df$Alpha, "\n(", alpha_dist_df$Alpha_Type, ")"),
        main = "Alpha Parameter Selection Distribution",
        xlab = "Alpha Value",
        ylab = "Frequency (Number of Folds)",
        col = bar_colors,
        border = "black",
        ylim = c(0, max(alpha_dist_df$Count) * 1.2))

# 添加数值标签
text(x = seq_along(alpha_dist_df$Count) * 1.2 - 0.5,
     y = alpha_dist_df$Count,
     labels = alpha_dist_df$Count,
     pos = 3, cex = 1.2, font = 2)

dev.off()

cat("Alpha参数筛选图已保存（TIFF格式，600 DPI，灰度）\n")

# 如果只用了二分类的glmnet，还可以展示lambda vs alpha的散点图（灰度）
if (num_class == 2) {
  cat("\n生成Alpha-Lambda散点图...\n")
  
  # 创建最佳参数分布图
  best_params_df <- do.call(rbind, lapply(seq_along(all_best_params), function(i) {
    data.frame(
      Fold = paste0("Fold", i),
      Alpha = all_best_params[[i]]$alpha,
      Lambda = all_best_params[[i]]$lambda,
      Performance = all_metrics[[i]][[SELECTION_METRIC]]
    )
  }))
  
  # 保存最佳参数数据
  write_xlsx(list(BestParameters = best_params_df), 
            path = file.path(OUTPUT_DIR, "best_parameters_per_fold.xlsx"))
  
  # 绘制散点图：Alpha vs Lambda（灰度）
  tiff(file.path(OUTPUT_DIR, "alpha_lambda_scatter.tiff"), 
       width = 8, height = 6, units = "in", res = 600, compression = "lzw")
  par(mar = c(5, 5, 4, 2))
  
  # 根据性能映射点的大小和灰度
  point_sizes <- best_params_df$Performance * 10
  point_colors <- gray.colors(CV_FOLDS, start = 0.2, end = 0.7)[rank(best_params_df$Performance)]
  
  plot(best_params_df$Alpha, 
       log10(best_params_df$Lambda),
       pch = 21,
       bg = point_colors,
       col = "black",
       cex = point_sizes / 5,
       main = "Best Parameters per Fold",
       xlab = "Alpha (0=Ridge, 1=Lasso)",
       ylab = "log10(Lambda)",
       xlim = c(-0.1, 1.1))
  
  # 添加网格
  grid(col = "grey80", lty = "dotted")
  
  # 添加alpha类型标签
  abline(v = c(0, 1), lty = 2, col = "grey50")
  text(0, par("usr")[3], "Ridge", pos = 1, cex = 0.8)
  text(1, par("usr")[3], "Lasso", pos = 1, cex = 0.8)
  
  # 添加图例
  legend("topright", 
         legend = c("Larger = Better Performance"),
         pch = 21, pt.bg = "grey50", pt.cex = 2,
         bty = "n")
  
  dev.off()
  
  cat("Alpha-Lambda散点图已保存（TIFF格式，600 DPI，灰度）\n")
}

# ==================== 新增：保存早停诊断 ====================
if (ENABLE_EARLY_STOPPING && SAVE_EARLY_STOP_DIAGNOSTICS) {
  # 为最佳折叠绘制早停诊断图
  plot_early_stop_diagnostics(
    all_early_stop_info[[best_fold_idx]]$early_stop_history, 
    OUTPUT_DIR
  )
  
  # 保存早停统计信息
  early_stop_summary <- data.frame(
    Fold = 1:CV_FOLDS,
    StoppedEarly = sapply(all_early_stop_info, function(x) x$stopped_early),
    StopIteration = sapply(all_early_stop_info, function(x) {
      if (is.null(x$stop_iteration)) {
        return(NA_integer_)
      } else {
        return(as.integer(x$stop_iteration))
      }
    }),
    TotalIterations = sapply(all_early_stop_info, function(x) as.integer(x$total_iterations)),
    ParametersSaved = sapply(all_early_stop_info, function(x) {
      as.integer(nrow(LR_GRID) - x$total_iterations)
    })
  )
  
  write_xlsx(list(EarlyStopSummary = early_stop_summary), 
            path = file.path(OUTPUT_DIR, "early_stop_summary.xlsx"))
  
  cat("\n早停统计:\n")
  print(early_stop_summary)
}

# ==================== 特征重要性分析（系数，灰度）====================
if (num_class > 2) {
  # 多分类：提取系数
  coef_matrix <- summary(best_model_final)$coefficients
  importance_df <- data.frame(
    Feature = colnames(X),
    Coefficient = abs(coef_matrix[-1, 1])  # 去除截距，取第一个类别
  )
} else {
  # 二分类：提取系数
  coef_values <- as.vector(coef(best_model_final, s = best_params_for_lc$lambda))
  importance_df <- data.frame(
    Feature = c("Intercept", colnames(X)),
    Coefficient = abs(coef_values)
  )
  # 移除截距
  importance_df <- importance_df[importance_df$Feature != "Intercept", ]
}

# 按重要性排序
importance_df <- importance_df[order(-importance_df$Coefficient), ]
rownames(importance_df) <- NULL

# 绘制特征重要性图（灰度）
tiff(file.path(OUTPUT_DIR, "feature_importance.tiff"), 
     width = 8, height = 6, units = "in", res = 600, compression = "lzw")
par(mar = c(5, 8, 4, 2))

# 创建灰度渐变（从深灰到浅灰）
n_features <- min(20, nrow(importance_df))
colors <- gray.colors(n_features, start = 0.2, end = 0.8, rev = TRUE)

barplot(importance_df$Coefficient[1:n_features], 
        names.arg = importance_df$Feature[1:n_features],
        horiz = TRUE, las = 1,
        main = gsub("_", " ", basename(OUTPUT_DIR)),
        xlab = "Absolute Coefficient Value",
        col = colors,
        border = "black")
dev.off()
cat("特征重要性图已保存（TIFF格式，600 DPI，灰度）\n")

# 保存特征重要性数据到Excel
write_xlsx(list(Importance = importance_df), 
           path = file.path(OUTPUT_DIR, "feature_importance.xlsx"))

# ==================== 新增：特征选择可视化（灰度）====================
cat("\n生成特征选择可视化...\n")

# 提取所有折的系数矩阵
feature_names <- colnames(X)
n_features <- length(feature_names)

# 创建特征选择矩阵（每行=特征，每列=折）
feature_selection_matrix <- matrix(0, nrow = n_features, ncol = CV_FOLDS)
rownames(feature_selection_matrix) <- feature_names
colnames(feature_selection_matrix) <- paste0("Fold", 1:CV_FOLDS)

# 填充矩阵：1=被选中，0=未被选中
for (fold in 1:CV_FOLDS) {
  coefs <- all_coefficients[[fold]]
  # 根据阈值判断是否被选中
  selected <- abs(coefs) > COEFFICIENT_THRESHOLD
  feature_selection_matrix[, fold] <- as.integer(selected)
}

# 计算每个特征的选中率
selection_rate <- rowMeans(feature_selection_matrix)
selection_count <- rowSums(feature_selection_matrix)

# 创建特征选择数据框
feature_selection_df <- data.frame(
  Feature = feature_names,
  Fold1 = feature_selection_matrix[, 1],
  Fold2 = feature_selection_matrix[, 2],
  Fold3 = feature_selection_matrix[, 3],
  Fold4 = feature_selection_matrix[, 4],
  Fold5 = feature_selection_matrix[, 5],
  Count = selection_count,
  Rate = selection_rate,
  Status = ifelse(selection_rate >= FEATURE_SELECTION_THRESHOLD, "Selected", "Not Selected")
)

# 按选中率降序排序
feature_selection_df <- feature_selection_df[order(-feature_selection_df$Rate), ]
rownames(feature_selection_df) <- NULL

# 保存特征选择数据
write_xlsx(list(FeatureSelection = feature_selection_df), 
          path = file.path(OUTPUT_DIR, "feature_selection.xlsx"))

# 筛选出被选中的特征
selected_features <- feature_selection_df[feature_selection_df$Status == "Selected", ]

# 保存仅包含被选特征名称的列表（供后续使用）
if (nrow(selected_features) > 0) {
  write_xlsx(list(SelectedFeatures = data.frame(Feature = selected_features$Feature)), 
            path = file.path(OUTPUT_DIR, "selected_features_list.xlsx"))
}

cat(sprintf("特征选择阈值: %.0f%% (至少%d/%d折)\n", 
           FEATURE_SELECTION_THRESHOLD * 100, 
           ceiling(FEATURE_SELECTION_THRESHOLD * CV_FOLDS), 
           CV_FOLDS))
cat(sprintf("被选中特征数: %d/%d\n", nrow(selected_features), n_features))

# ==================== 新增：特征选择热力图（灰度）====================
# 准备热力图数据
heatmap_data <- feature_selection_matrix
heatmap_data <- heatmap_data[order(-rowSums(heatmap_data)), ]  # 按选中次数排序

# 转换为长格式
heatmap_df <- melt(heatmap_data)
colnames(heatmap_df) <- c("Feature", "Fold", "Selected")

# 绘制热力图（TIFF格式，600 DPI，灰度）
tiff(file.path(OUTPUT_DIR, "feature_selection_heatmap.tiff"), 
     width = 10, height = max(6, nrow(heatmap_data) * 0.2), 
     units = "in", res = 600, compression = "lzw")

p <- ggplot(heatmap_df, aes(x = Fold, y = Feature, fill = factor(Selected))) +
  geom_tile(color = "white", size = 0.5) +
  scale_fill_manual(
    values = c("0" = "grey90", "1" = "grey30"),
    labels = c("Not Selected", "Selected"),
    name = "Selection Status"
  ) +
  labs(
    title = "Feature Selection Across CV Folds",
    x = "Cross-Validation Fold",
    y = "Feature"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    axis.text.x = element_text(angle = 0, hjust = 0.5),
    axis.text.y = element_text(size = 8),
    legend.position = "bottom",
    panel.grid = element_blank()
  )

print(p)
dev.off()

cat("特征选择热力图已保存（TIFF格式，600 DPI，灰度）\n")

# ==================== 新增：特征选择率柱状图（灰度）====================
# 准备柱状图数据
bar_data <- feature_selection_df[order(-feature_selection_df$Rate), ]
bar_data$Feature <- factor(bar_data$Feature, levels = bar_data$Feature)  # 保持排序

# 添加颜色标记（是否被选中）
bar_data$IsSelected <- bar_data$Rate >= FEATURE_SELECTION_THRESHOLD

tiff(file.path(OUTPUT_DIR, "feature_selection_rates.tiff"), 
     width = 12, height = max(6, nrow(bar_data) * 0.15), 
     units = "in", res = 600, compression = "lzw")

p <- ggplot(bar_data, aes(x = Feature, y = Rate, fill = IsSelected)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = FEATURE_SELECTION_THRESHOLD, 
            linetype = "dashed", color = "black", size = 1) +
  scale_fill_manual(
    values = c("TRUE" = "grey30", "FALSE" = "grey70"),
    labels = c("Not Selected", "Selected"),
    name = "Selection Status"
  ) +
  labs(
    title = sprintf("Feature Selection Rate Across %d-Fold CV", CV_FOLDS),
    x = "Feature",
    y = "Selection Rate",
    caption = sprintf("Dashed line indicates selection threshold (%.0f%%)", 
                     FEATURE_SELECTION_THRESHOLD * 100)
  ) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 10),
    legend.position = "bottom",
    panel.grid.minor = element_blank(),
    plot.caption = element_text(hjust = 0.5, face = "italic")
  )

print(p)
dev.off()

cat("特征选择率柱状图已保存（TIFF格式，600 DPI，灰度）\n")

# ==================== 绘制特征选择条形图（灰度）====================
if (nrow(selected_features) > 0) {
  tiff(file.path(OUTPUT_DIR, "feature_selection.tiff"), 
       width = 8, height = max(6, nrow(selected_features) * 0.3), 
       units = "in", res = 600, compression = "lzw")
  
  par(mar = c(5, 10, 4, 2))
  
  # 根据选中率分配灰度
  # 100% -> 最深灰 (grey20)
  # 阈值 -> 最浅灰 (grey70)
  get_gray_color <- function(rate) {
    if (rate == 1.0) return("grey20")
    if (rate >= 0.8) return("grey35")
    if (rate >= FEATURE_SELECTION_THRESHOLD) return("grey50")
    return("grey70")
  }
  
  colors <- sapply(selected_features$Rate, get_gray_color)
  
  # 反转顺序，使100%在上，阈值在下
  selected_features_reversed <- selected_features[nrow(selected_features):1, ]
  colors_reversed <- rev(colors)
  
  # 绘制横向条形图
  barplot(selected_features_reversed$Rate * 100,
          names.arg = selected_features_reversed$Feature,
          horiz = TRUE,
          las = 1,
          xlim = c(0, 105),
          main = "",  # 主标题留空，后面单独添加
          xlab = "",
          ylab = "",
          col = colors_reversed,
          border = "black",
          cex.lab = 1.1,
          font.lab = 2)
  
  # 使用外标题选项
  title(main = "Feature Selection Stability", 
        outer = TRUE,  # 在整个图形区域绘制
        line = -2,      # 调整垂直位置
        cex.main = 1.4, 
        font.main = 2,
        adj = 0.49)     # 稍微偏左调整（0.48而不是0.5）

  # 副标题
  title(main = sprintf("(n=%d, Threshold ≥%.0f%%)", 
                      nrow(selected_features),
                      FEATURE_SELECTION_THRESHOLD * 100),
        outer = TRUE,
        line = -3,    # 调整垂直位置
        cex.main = 0.9, 
        font.main = 1,
        adj = 0.49) 

  #添加横轴标签
  mtext("Selection Frequency (%)", 
      side = 1,      # 1 = 下方
      line = -2,      # 调整距离
      cex = 1.1,
      font = 2,
      outer = TRUE)  # 在整个图像区域  

  #添加纵轴标签
   mtext("Selected Features", 
      side = 2,      # 2 表示左侧
      line = 8,      # 调整这个值控制左右位置
      cex = 1.1,
      font = 2,
      las = 0)       # 保持文字方向
  
  # 添加数值标签
  text(x = selected_features_reversed$Rate * 100 + 3,
       y = seq_along(selected_features_reversed$Rate) * 1.2 - 0.5,
       labels = sprintf("%.0f%% (%d/%d)", 
                       selected_features_reversed$Rate * 100,
                       selected_features_reversed$Count,
                       CV_FOLDS),
       pos = 4, cex = 0.8)
  
  # 添加阈值线
  abline(v = FEATURE_SELECTION_THRESHOLD * 100, lty = 2, col = "black", lwd = 2)
  
  dev.off()
  
  cat("特征选择条形图已保存（TIFF格式，600 DPI，灰度）\n")
}

# ==================== 绘制LASSO系数路径图（5折组图，灰度）====================
if (num_class == 2) {
  cat("\n生成LASSO系数路径图...\n")
  
  # 收集所有折的lambda和系数数据
  coefficient_paths <- list()
  valid_folds <- c()  # 记录哪些折使用了LASSO/Elastic Net
  
  for (fold in 1:CV_FOLDS) {
    best_model_fold <- all_models[[fold]]
    best_alpha <- all_best_params[[fold]]$alpha
    
    # 只为使用LASSO或Elastic Net的折绘制路径
    if (best_alpha > 0) {
      # 获取完整的系数路径
      coef_matrix <- as.matrix(coef(best_model_fold))
      lambda_seq <- best_model_fold$lambda
      
      # 转换为数据框（去除截距）
      coef_df <- as.data.frame(t(coef_matrix[-1, ]))
      coef_df$lambda <- lambda_seq
      coef_df$fold <- fold
      
      coefficient_paths[[fold]] <- coef_df
      valid_folds <- c(valid_folds, fold)
    }
  }
  
  if (length(valid_folds) > 0) {
    # 合并所有折的数据
    all_paths <- do.call(rbind, coefficient_paths[valid_folds])
    
    # 保存系数路径数据
    write_xlsx(list(CoefficientPaths = all_paths), 
              path = file.path(OUTPUT_DIR, "coefficient_paths.xlsx"))
    
    # 确定布局（根据有效折数）
    n_valid <- length(valid_folds)
    if (n_valid <= 2) {
      layout_rows <- 1
      layout_cols <- n_valid
    } else if (n_valid <= 4) {
      layout_rows <- 2
      layout_cols <- 2
    } else {
      layout_rows <- 3
      layout_cols <- 2
    }
    
    # 绘制组图（灰度）
    tiff(file.path(OUTPUT_DIR, "coefficient_paths.tiff"), 
         width = 12, height = layout_rows * 3.5, units = "in", res = 600, compression = "lzw")
    
    par(mfrow = c(layout_rows, layout_cols), mar = c(4, 4, 3, 1), oma = c(2, 2, 3, 1))
    
    # 只绘制有效的折
    for (fold in valid_folds) {
      fold_data <- coefficient_paths[[fold]]
      
      # 准备绘图数据
      lambda_vals <- fold_data$lambda
      
      # 计算y轴范围
      coef_values <- as.matrix(fold_data[, 1:(ncol(fold_data)-2)])
      y_range <- range(coef_values)
      
      # 绘制每个特征的系数路径
      plot(NA, NA,
           xlim = range(log10(lambda_vals)),
           ylim = y_range,
           xlab = "log10(Lambda)",
           ylab = "Coefficients",
           main = sprintf("Fold %d (α=%.1f)", fold, all_best_params[[fold]]$alpha))
      
      # 添加网格
      grid(col = "grey90", lty = "dotted")
      
      # 为每个特征绘制路径
      n_features_plot <- ncol(fold_data) - 2
      for (feat_idx in 1:n_features_plot) {
        # 使用不同灰度表示不同特征
        gray_val <- 0.2 + (feat_idx %% 5) * 0.15
        lines(log10(lambda_vals), fold_data[, feat_idx], 
              col = grey(gray_val), lwd = 1.5)
      }
      
      # 标记最佳lambda
      best_lambda <- all_best_params[[fold]]$lambda
      abline(v = log10(best_lambda), lty = 2, col = "black", lwd = 2)
      text(log10(best_lambda), par("usr")[4] * 0.9, 
           "Best λ", pos = 4, cex = 0.7, font = 2)
    }
    
    # 如果还有空位，在最后一个位置放置说明
    n_plots <- layout_rows * layout_cols
    if (n_valid < n_plots) {
      plot.new()
      legend("center", 
             legend = c("Each line = one feature",
                       "Dashed line = selected λ",
                       "Different shades = different features"),
             bty = "n", cex = 1.2)
    }
    
    # 添加总标题
    mtext("LASSO Coefficient Paths Across Folds", 
          outer = TRUE, cex = 1.5, font = 2, line = 0.5)
    
    dev.off()
    
    cat(sprintf("LASSO系数路径图已保存（TIFF格式，600 DPI，灰度）- 共%d个有效折\n", n_valid))
  } else {
    cat("注意: 所有折都使用Ridge回归(α=0)，无法生成LASSO系数路径图\n")
  }
}

# ==================== 保存optimal_parameters.txt ====================
optimal_params_file <- file.path(OUTPUT_DIR, "optimal_parameters.txt")
sink(optimal_params_file)
cat("# 最优逻辑回归参数 #\n\n")
cat("特征选择方法: 原始数据\n")
cat("模型选择指标:", SELECTION_METRIC, "\n")
if (ENABLE_EARLY_STOPPING) {
  cat("早停功能: 启用\n")
  cat("早停监控指标:", EARLY_STOP_METRIC, "\n")
  cat("过拟合阈值:", OVERFITTING_THRESHOLD, "\n")
  cat("早停耐心值:", EARLY_STOP_PATIENCE, "\n")
}
cat("\n参数值:\n")
best_params_final <- all_best_params[[best_fold_idx]]
for (param_name in names(best_params_final)) {
  cat(sprintf("%s = %s\n", param_name, best_params_final[[param_name]]))
}

# 定义要检查的指标
metrics_to_check <- c(
  "Precision" = "精确率 (Precision)",
  "Recall" = "召回率 (Recall)",
  "F1" = "F1分数",
  "Kappa" = "Kappa系数",
  "Mean_Sensitivity" = "平均敏感度",
  "Mean_Specificity" = "平均特异度",
  "Mean_Pos_Pred_Value" = "平均正预测值",
  "Mean_Neg_Pred_Value" = "平均负预测值",
  "Mean_Detection_Rate" = "平均检测率",
  "Mean_Balanced_Accuracy" = "平均平衡准确率"
)

# 输出平均指标
cat("\n平均性能指标:\n")
cat(sprintf("准确率 (Accuracy): %.4f\n", avg_metrics$Accuracy))

# 输出其他指标
for (metric in names(metrics_to_check)) {
  if (metric %in% names(avg_metrics)) {
    cat(sprintf("%s: %.4f\n", metrics_to_check[metric], avg_metrics[[metric]]))
  }
}

# 输出AUC
cat(sprintf("AUC: %.4f\n", avg_metrics$AUC))

# 输出所有原始特征
cat("\n原始特征 (共", ncol(X), "个):\n")
cat(paste(colnames(X), collapse = ", "), "\n")

# 新增：输出被选中的特征
if (exists("selected_features") && nrow(selected_features) > 0) {
  cat("\n## 特征选择结果 ##\n")
  cat(sprintf("选择阈值: %.0f%% (至少在%d/%d折中被选中)\n",
             FEATURE_SELECTION_THRESHOLD * 100,
             ceiling(FEATURE_SELECTION_THRESHOLD * CV_FOLDS),
             CV_FOLDS))
  cat(sprintf("系数阈值: %.2e (|系数| > 此值视为被选中)\n", COEFFICIENT_THRESHOLD))
  cat(sprintf("\n被选中的特征 (共%d个，占%.1f%%):\n", 
             nrow(selected_features), 
             nrow(selected_features) / ncol(X) * 100))
  
  for (i in 1:nrow(selected_features)) {
    cat(sprintf("  %d. %s - 选中率: %.0f%% (%d/%d折)\n",
               i,
               selected_features$Feature[i],
               selected_features$Rate[i] * 100,
               selected_features$Count[i],
               CV_FOLDS))
  }
  
  cat("\n这些特征可用于后续模型训练（如树模型、神经网络等）\n")
  cat("特征列表已保存至: selected_features_list.xlsx\n")
}

# 添加最佳折叠信息
cat("\n最佳模型来自：Fold", best_fold_idx, "\n")
cat(sprintf("最佳折叠%s：%.4f\n", SELECTION_METRIC, fold_scores[best_fold_idx]))

# 添加过拟合诊断信息
cat("\n## 过拟合诊断 (学习曲线分析) ##\n")
for (ratio in names(learning_curve_results)) {
  result <- learning_curve_results[[ratio]]
  cat(sprintf("样本比例 %.1f%% (n=%d): 训练AUC=%.4f, 验证AUC=%.4f, 差距=%.4f\n",
              as.numeric(ratio) * 100, result$sample_size, 
              result$train_auc, result$val_auc, result$gap))
}

# 新增：早停诊断信息
if (ENABLE_EARLY_STOPPING) {
  cat("\n## 早停诊断信息 ##\n")
  
  # 安全地计算统计信息
  total_stopped <- sum(sapply(all_early_stop_info, function(x) {
    if (is.null(x) || is.null(x$stopped_early)) return(FALSE)
    return(x$stopped_early)
  }))
  
  total_saved <- sum(sapply(all_early_stop_info, function(x) {
    if (is.null(x) || is.null(x$total_iterations)) return(0)
    return(nrow(LR_GRID) - x$total_iterations)
  }))
  
  cat(sprintf("启用早停的折数: %d/%d\n", total_stopped, CV_FOLDS))
  
  if (total_saved > 0) {
    cat(sprintf("总共节省的参数组合数: %d (%.1f%%)\n", 
               total_saved, 100 * total_saved / (CV_FOLDS * nrow(LR_GRID))))
    
    for (fold in 1:CV_FOLDS) {
      info <- all_early_stop_info[[fold]]
      if (!is.null(info) && !is.null(info$stopped_early) && info$stopped_early) {
        cat(sprintf("  Fold%d: 在第%d/%d个参数组合处停止，节省%d个组合\n",
                   fold, info$stop_iteration, nrow(LR_GRID),
                   nrow(LR_GRID) - info$stop_iteration))
      }
    }
  } else {
    cat("所有折都完成了全部参数搜索（未触发早停）\n")
  }
}

sink()

# ==================== 保存结果到RDS ====================
results <- list(
  selected_features = colnames(X),
  feature_scores = setNames(rep(1, ncol(X)), colnames(X)),
  best_params = best_params_final,
  metrics = avg_metrics,
  predictions = all_predictions,
  models = all_models,
  early_stop_info = all_early_stop_info  # 新增
)

saveRDS(results, file = file.path(OUTPUT_DIR, "feature_selection_results.rds"))

cat("\n==================== 训练完成 ====================\n")
cat("所有结果已保存到:", OUTPUT_DIR, "\n")
if (ENABLE_EARLY_STOPPING) {
  total_saved <- sum(sapply(all_early_stop_info, function(x) {
    if (is.null(x) || is.null(x$total_iterations)) return(0)
    return(nrow(LR_GRID) - x$total_iterations)
  }))
  
  if (total_saved > 0) {
    cat(sprintf("\n早停功能已启用，总共节省了约 %.1f%% 的计算时间\n",
               100 * total_saved / (CV_FOLDS * nrow(LR_GRID))))
  } else {
    cat("\n早停功能已启用，但所有折都完成了全部参数搜索\n")
  }
}